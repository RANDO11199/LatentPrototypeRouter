model_name: "MixtralLPR-0.6B-0.1b"

model_type: "MixtralLPR"

eval: false

model_config:
  vocab_size: 151936
  hidden_size: 1024
  intermediate_size: 128
  moe_intermediate_size: 128
  num_hidden_layers: 6
  num_attention_heads: 16
  num_key_value_heads: 4
  num_local_experts: 128
  num_experts_per_tok: 8
  output_router_logits: true
  hidden_act: "silu"
  max_position_embeddings: 4096
  router_aux_loss_coef: 0.0
  output_router_kl: true
  router_kl_loss_coef: 0.01
  router_latent_dim: 16
  router_ema_decay: 0.8
  diversity_lambda: 1.0
  diversity_type: "orthogonal"
  SimilarityMetrics: "VectorSimilarity"
  routing_metrics: "Cosine"
  kl_weight: 0.01
  align_weight: 0.05
  div_weight: 1.0
  unit_ball: false
  
training_config:
  total_steps: 4000
  warmup_ratio: 0.05
  stable_ratio: 0.75
  max_length: 1024
  adam_beta1: 0.9
  adam_beta2: 0.95
  weight_decay: 0.1
  lr: 0.001
  min_lr_ratio: 0.05
  gradient_clip_norm: 1.0
  gradient_accumulate_steps: 27
  per_device_train_batch_size: 10
  per_device_eval_batch_size: 10
